{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "09bfec57-2e37-43aa-94db-d703d027fcdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp\n",
    "import kfp.dsl as dsl\n",
    "from kfp.v2.dsl import (\n",
    "    component,\n",
    "    Input,\n",
    "    Output,\n",
    "    Dataset,\n",
    "    Metrics,\n",
    "    Artifact,\n",
    "    OutputPath,\n",
    "    InputPath,\n",
    "    ClassificationMetrics,\n",
    "    Model,\n",
    ")\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5e30870",
   "metadata": {},
   "source": [
    "## Download_csv\n",
    "\n",
    "This component downloads the dataset from the given url and outputs a csv file for downstream components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74c90c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    base_image='yinanli617/customer-churn:latest',\n",
    "    output_component_file='./components/download_csv.yaml',\n",
    ")\n",
    "def download_csv(url: str, output_csv: Output[Dataset]):\n",
    "    import urllib.request\n",
    "    import pandas as pd\n",
    "    \n",
    "    urllib.request.urlretrieve(url=url,\n",
    "                               filename=output_csv.path,\n",
    "                              )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e61903a1",
   "metadata": {},
   "source": [
    "## Train_test_split\n",
    "\n",
    "This component splits the original dataset to a training set and a test set. The test set is meant to be used to evaluate the performance of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1995cff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    base_image='yinanli617/customer-churn:latest',\n",
    "    output_component_file='./components/train_test_split.yaml',\n",
    ")\n",
    "def train_test_split(input_csv: Input[Dataset],\n",
    "                     seed: int,\n",
    "                     target: str,\n",
    "                     train_csv: Output[Dataset],\n",
    "                     test_csv: Output[Dataset]\n",
    "                    ):\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    import pandas as pd\n",
    "    \n",
    "    df = pd.read_csv(input_csv.path)\n",
    "    train, test = train_test_split(df,\n",
    "                                   test_size=0.2,\n",
    "                                   shuffle=True,\n",
    "                                   random_state=seed,\n",
    "                                   stratify=df[target],\n",
    "                                  )\n",
    "    train_df = pd.DataFrame(train)\n",
    "    train_df.columns = df.columns\n",
    "    test_df = pd.DataFrame(test)\n",
    "    test_df.columns = df.columns\n",
    "    \n",
    "    train_df.to_csv(train_csv.path, index=False)\n",
    "    test_df.to_csv(test_csv.path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "686f6c4f",
   "metadata": {},
   "source": [
    "## Preprecessing\n",
    "\n",
    "In this step, we preprocess the training dataset so that we can feed the data later on to our models. Specifically, we use a `OneHotEncoder` to encode the `categorical_features` and a `StandardScaler` to standardize the `numerical_features`. We will fit the encoder and the scaler to the training data, and save them as outputs which will be used to transform the test dataset later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3935da97",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    base_image='yinanli617/customer-churn:latest',\n",
    "    output_component_file='./components/preprocessing.yaml',\n",
    ")\n",
    "def preprocessing(input_csv: Input[Dataset],\n",
    "                  numerical_features: str,\n",
    "                  categorical_features: str,\n",
    "                  target: str,\n",
    "                  features: Output[Dataset],\n",
    "                  labels: Output[Dataset],\n",
    "                  scaler_obj: Output[Model],\n",
    "                  encoder_obj: Output[Model],\n",
    "                 ):\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "    from pickle import dump\n",
    "    from ast import literal_eval\n",
    "    \n",
    "    # The features are stored as strings. We do the trick with literal_eval to convert them to the\n",
    "    # correct format (list)\n",
    "    categorical_features = literal_eval(categorical_features)\n",
    "    numerical_features = literal_eval(numerical_features)\n",
    "    \n",
    "    df = pd.read_csv(input_csv.path)\n",
    "    X_cat = df[categorical_features]\n",
    "    X_num = df[numerical_features]\n",
    "    y = df[target]\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X_num = scaler.fit_transform(X_num)\n",
    "    encoder = OneHotEncoder()\n",
    "    X_cat = encoder.fit_transform(X_cat).toarray()\n",
    "    X = np.concatenate([X_num, X_cat], axis=1)\n",
    "    \n",
    "    pd.DataFrame(X).to_csv(features.path, index=False)\n",
    "    y.to_csv(labels.path, index=False)\n",
    "    # To prevent leakage, the scaler and the encoder should not see the test dataset.\n",
    "    # We save the scaler and encoder that have been fit to the training dataset and \n",
    "    # use it directly on the test dataset later on.\n",
    "    dump(scaler, open(scaler_obj.path, 'wb'))\n",
    "    dump(encoder, open(encoder_obj.path, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe1584fe",
   "metadata": {},
   "source": [
    "## Train base line models\n",
    "\n",
    "Here we feed out preprocessed data to 3 base line models - logistic regression, random forests, and K nearest neighbors. We perform cross validation with each model and use grid search to find the optimal hyperparameters. The number of folds for CV and the hyperparameter candidates are parameters to be fed to the pipeline. Each model will output a summary of the CV results as well as the model. They also output the selected metrics so that we can compare model performances in the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2dd30512",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    base_image='yinanli617/customer-churn:latest',\n",
    "    output_component_file='./components/logistic_regression.yaml',\n",
    ")\n",
    "def logistic_regression(features: Input[Dataset],\n",
    "                        labels: Input[Dataset],\n",
    "                        param_grid: str,\n",
    "                        num_folds: int,\n",
    "                        scoring: str,\n",
    "                        seed: int,\n",
    "                        best_model: Output[Model],\n",
    "                        best_params: Output[Dataset],\n",
    "                        best_score: Output[Metrics],\n",
    "                        cv_results: Output[Dataset],\n",
    "                       ) -> float:\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    import pandas as pd\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "    from sklearn.model_selection import GridSearchCV\n",
    "    from pickle import dump\n",
    "    from ast import literal_eval\n",
    "    \n",
    "    lr = LogisticRegression(solver='liblinear',\n",
    "                            random_state=seed,\n",
    "                           )\n",
    "    param_grid = literal_eval(param_grid)\n",
    "    grid_search = GridSearchCV(lr,\n",
    "                               param_grid=param_grid,\n",
    "                               scoring=scoring,\n",
    "                               refit=True, # Use the whole dataset to retrain after finding the best params\n",
    "                               cv=num_folds,\n",
    "                               verbose=2,\n",
    "                              )\n",
    "    \n",
    "    X, y = pd.read_csv(features.path).values, pd.read_csv(labels.path).values\n",
    "    grid_search.fit(X, y)\n",
    "    \n",
    "    pd.DataFrame(grid_search.cv_results_).to_csv(cv_results.path, index=False)\n",
    "    best_params_ = grid_search.best_params_\n",
    "    for key, value in best_params_.items():\n",
    "        best_params_[key] = [value]\n",
    "    pd.DataFrame(best_params_).to_csv(best_params.path, index=False)\n",
    "    dump(grid_search.best_estimator_, open(best_model.path, 'wb'))\n",
    "    best_score.log_metric(scoring, grid_search.best_score_)\n",
    "    \n",
    "    return grid_search.best_score_\n",
    "    \n",
    "    \n",
    "@component(\n",
    "    base_image='yinanli617/customer-churn:latest',\n",
    "    output_component_file='./components/random_forests.yaml',\n",
    ")\n",
    "def random_forests(features: Input[Dataset],\n",
    "                   labels: Input[Dataset],\n",
    "                   param_grid: str,\n",
    "                   num_folds: int,\n",
    "                   scoring: str,\n",
    "                   seed: int,\n",
    "                   best_model: Output[Model],\n",
    "                   best_params: Output[Dataset],\n",
    "                   best_score: Output[Metrics],\n",
    "                   cv_results: Output[Dataset],\n",
    "                  ) -> float:\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    import pandas as pd\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "    from sklearn.model_selection import GridSearchCV\n",
    "    from pickle import dump\n",
    "    from ast import literal_eval\n",
    "    \n",
    "    rf = RandomForestClassifier(random_state=seed)\n",
    "    param_grid = literal_eval(param_grid)\n",
    "    grid_search = GridSearchCV(rf,\n",
    "                               param_grid=param_grid,\n",
    "                               scoring=scoring,\n",
    "                               refit=True, # Use the whole dataset to retrain after finding the best params\n",
    "                               cv=num_folds,\n",
    "                               verbose=2,\n",
    "                              )\n",
    "    \n",
    "    X, y = pd.read_csv(features.path).values, pd.read_csv(labels.path).values\n",
    "    grid_search.fit(X, y)\n",
    "    \n",
    "    pd.DataFrame(grid_search.cv_results_).to_csv(cv_results.path, index=False)\n",
    "    best_params_ = grid_search.best_params_\n",
    "    for key, value in best_params_.items():\n",
    "        best_params_[key] = [value]\n",
    "    pd.DataFrame(best_params_).to_csv(best_params.path, index=False)\n",
    "    dump(grid_search.best_estimator_, open(best_model.path, 'wb'))\n",
    "    best_score.log_metric(scoring, grid_search.best_score_)\n",
    "    \n",
    "    return grid_search.best_score_\n",
    "\n",
    "\n",
    "@component(\n",
    "    base_image='yinanli617/customer-churn:latest',\n",
    "    output_component_file='./components/k_nearest_neighbors.yaml',\n",
    ")\n",
    "def knn(features: Input[Dataset],\n",
    "        labels: Input[Dataset],\n",
    "        param_grid: str,\n",
    "        num_folds: int,\n",
    "        scoring: str,\n",
    "        best_model: Output[Model],\n",
    "        best_params: Output[Dataset],\n",
    "        best_score: Output[Metrics],\n",
    "        cv_results: Output[Dataset],\n",
    "       ) -> float:\n",
    "    from sklearn.neighbors import KNeighborsClassifier\n",
    "    import pandas as pd\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "    from sklearn.model_selection import GridSearchCV\n",
    "    from pickle import dump\n",
    "    from ast import literal_eval\n",
    "    \n",
    "    k_nn = KNeighborsClassifier()\n",
    "    param_grid = literal_eval(param_grid)\n",
    "    grid_search = GridSearchCV(k_nn,\n",
    "                               param_grid=param_grid,\n",
    "                               scoring=scoring,\n",
    "                               refit=True, # Use the whole dataset to retrain after finding the best params\n",
    "                               cv=num_folds,\n",
    "                               verbose=2,\n",
    "                              )\n",
    "    \n",
    "    X, y = pd.read_csv(features.path).values, pd.read_csv(labels.path).values\n",
    "    grid_search.fit(X, y)\n",
    "    \n",
    "    pd.DataFrame(grid_search.cv_results_).to_csv(cv_results.path, index=False)\n",
    "    best_params_ = grid_search.best_params_\n",
    "    for key, value in best_params_.items():\n",
    "        best_params_[key] = [value]\n",
    "    pd.DataFrame(best_params_).to_csv(best_params.path, index=False)\n",
    "    dump(grid_search.best_estimator_, open(best_model.path, 'wb'))\n",
    "    best_score.log_metric(scoring, grid_search.best_score_)\n",
    "    \n",
    "    return grid_search.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81db899f",
   "metadata": {},
   "source": [
    "## Evaluate performance with test dataset\n",
    "\n",
    "Finally, in this step we compare the metrics from the 3 baseline models. We select the best model and use it to predict the unseen test dataset. We also use the previously saved scaler and encoder to preprocess the test dataset before making the predictions. We output different evaluation metrics which can be visualized in the Kubeflow Pipeline UI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "363e8ab4-afc3-4817-854c-c64a64bcf600",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    base_image='yinanli617/customer-churn:latest',\n",
    "    output_component_file='./components/predict_test_data.yaml',\n",
    ")\n",
    "def predict_test_data(test_csv: Input[Dataset],\n",
    "                      scaler_obj: Input[Model],\n",
    "                      encoder_obj: Input[Model],\n",
    "                      lr_model: Input[Model],\n",
    "                      rf_model: Input[Model],\n",
    "                      knn_model: Input[Model],\n",
    "                      lr_score: float,\n",
    "                      rf_score: float,\n",
    "                      knn_score: float,\n",
    "                      categorical_features: str,\n",
    "                      numerical_features: str,\n",
    "                      target: str,\n",
    "                      metrics: Output[Metrics],\n",
    "                     ):\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from pickle import load\n",
    "    from ast import literal_eval\n",
    "    from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score, roc_auc_score\n",
    "    \n",
    "    categorical_features = literal_eval(categorical_features)\n",
    "    numerical_features = literal_eval(numerical_features)\n",
    "    \n",
    "    df = pd.read_csv(test_csv.path)\n",
    "    X_cat = df[categorical_features]\n",
    "    X_num = df[numerical_features]\n",
    "    y = df[target]\n",
    "    \n",
    "    scaler = load(open(scaler_obj.path, 'rb'))\n",
    "    X_num = scaler.transform(X_num)\n",
    "    encoder = load(open(encoder_obj.path, 'rb'))\n",
    "    X_cat = encoder.transform(X_cat).toarray()\n",
    "    X = np.concatenate([X_num, X_cat], axis=1)\n",
    "    \n",
    "    models_dict = {lr_score: lr_model,\n",
    "                   rf_score: rf_model,\n",
    "                   knn_score: knn_model,\n",
    "                  }\n",
    "    best_model = models_dict[max(models_dict.keys())]\n",
    "    model = load(open(best_model.path, 'rb'))\n",
    "    \n",
    "    y_pred = model.predict(X)\n",
    "    y_proba = model.predict_proba(X)[:, 1]\n",
    "    \n",
    "    accuracy = accuracy_score(y, y_pred)\n",
    "    f1 = f1_score(y, y_pred)\n",
    "    recall = recall_score(y, y_pred)\n",
    "    precision = precision_score(y, y_pred)\n",
    "    roc_auc = roc_auc_score(y, y_proba)\n",
    "    \n",
    "    metrics.log_metric('Accuracy', accuracy)\n",
    "    metrics.log_metric('F1 score', f1)\n",
    "    metrics.log_metric('Recall', recall)\n",
    "    metrics.log_metric('Precision', precision)\n",
    "    metrics.log_metric('ROC_AUC', roc_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9def2a4",
   "metadata": {},
   "source": [
    "## Assemble the pipeline with the defined components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f51c5f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.pipeline(\n",
    "    name='bank-customer-churn-pipeline',\n",
    "    # You can optionally specify your own pipeline_root\n",
    "    pipeline_root='gs://kfp-yli/customer-churn',\n",
    ")\n",
    "def my_pipeline(url: str,\n",
    "                num_folds: int,\n",
    "                target: str,\n",
    "                numerical_features: str,\n",
    "                categorical_features: str,\n",
    "                scoring: str,\n",
    "                logistic_regression_params: str,\n",
    "                random_forests_params: str,\n",
    "                knn_params: str,\n",
    "                seed: int,\n",
    "               ):\n",
    "\n",
    "    download_csv_task = download_csv(url=url)\n",
    "\n",
    "    train_test_split_task = train_test_split(input_csv=download_csv_task.outputs['output_csv'],\n",
    "                                             seed=seed,\n",
    "                                             target=target,\n",
    "                                            )\n",
    "    \n",
    "    train_preprocessing_task = preprocessing(input_csv=train_test_split_task.outputs['train_csv'],\n",
    "                                             numerical_features=numerical_features,\n",
    "                                             categorical_features=categorical_features,\n",
    "                                             target=target,\n",
    "                                            )\n",
    "    \n",
    "    logistic_regression_task = logistic_regression(features=train_preprocessing_task.outputs['features'],\n",
    "                                                   labels=train_preprocessing_task.outputs['labels'],\n",
    "                                                   scoring=scoring,\n",
    "                                                   seed=seed,\n",
    "                                                   num_folds=num_folds,\n",
    "                                                   param_grid=logistic_regression_params,\n",
    "                                                  )\n",
    "\n",
    "    random_forests_task = random_forests(features=train_preprocessing_task.outputs['features'],\n",
    "                                         labels=train_preprocessing_task.outputs['labels'],\n",
    "                                         scoring=scoring,\n",
    "                                         seed=seed,\n",
    "                                         num_folds=num_folds,\n",
    "                                         param_grid=random_forests_params,\n",
    "                                        )\n",
    "    \n",
    "    knn_task = knn(features=train_preprocessing_task.outputs['features'],\n",
    "                   labels=train_preprocessing_task.outputs['labels'],\n",
    "                   scoring=scoring,\n",
    "                   num_folds=num_folds,\n",
    "                   param_grid=knn_params,\n",
    "                  )\n",
    "    \n",
    "    predict_test_data_task = predict_test_data(test_csv=train_test_split_task.outputs['test_csv'],\n",
    "                                               scaler_obj=train_preprocessing_task.outputs['scaler_obj'],\n",
    "                                               encoder_obj=train_preprocessing_task.outputs['encoder_obj'],\n",
    "                                               lr_model=logistic_regression_task.outputs['best_model'],\n",
    "                                               rf_model=random_forests_task.outputs['best_model'],\n",
    "                                               knn_model=knn_task.outputs['best_model'],\n",
    "                                               lr_score=logistic_regression_task.outputs['output'],\n",
    "                                               rf_score=random_forests_task.outputs['output'],\n",
    "                                               knn_score=knn_task.outputs['output'],\n",
    "                                               categorical_features=categorical_features,\n",
    "                                               numerical_features=numerical_features,\n",
    "                                               target=target\n",
    "                                              )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "284d7b41",
   "metadata": {},
   "source": [
    "## Compile the pipeline\n",
    "\n",
    "The output YAML file can be uploaded from the Kubeflow Pipeline UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "230e9bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "kfp.compiler.Compiler(mode=kfp.dsl.PipelineExecutionMode.V2_COMPATIBLE).compile(\n",
    "    pipeline_func=my_pipeline,\n",
    "    package_path='./pipeline/customer-churn_pipeline.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca7898a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
